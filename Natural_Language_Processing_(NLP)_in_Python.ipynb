{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prabhu-patil/NLP-using-Python/blob/main/Natural_Language_Processing_(NLP)_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Text Preprocessing\n",
        "\n",
        "Proper text preprocessing is essential for effective NLP model development.\n",
        "**bold text**\n",
        "# 1.1 Tokenization\n",
        "\n",
        "Using nltk and spaCy for sentence and word tokenization. **bold text**"
      ],
      "metadata": {
        "id": "rZw70xKYsDb6"
      }
    },
    {
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import spacy\n",
        "\n",
        "# Download the 'punkt_tab' resource\n",
        "nltk.download('punkt_tab') # Downloading the missing resource for sentence tokenization.\n",
        "\n",
        "text = \"Natural Language Processing is amazing! Let's explore it.\"\n",
        "word_tokens = word_tokenize(text)\n",
        "sent_tokens = sent_tokenize(text)\n",
        "\n",
        "print(\"Word Tokens:\", word_tokens)\n",
        "print(\"Sentence Tokens:\", sent_tokens)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZvnaxOJr5cK",
        "outputId": "7c9c4179-aff6-4220-b020-acec2cde067a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Natural', 'Language', 'Processing', 'is', 'amazing', '!', 'Let', \"'s\", 'explore', 'it', '.']\n",
            "Sentence Tokens: ['Natural Language Processing is amazing!', \"Let's explore it.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.2 Stopword Removal**\n",
        "\n",
        "**Removing common words that do not contribute to meaning.**"
      ],
      "metadata": {
        "id": "dT7SHO3vsftB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in word_tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Filtered Tokens:\", filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqvqnA4a-3PX",
        "outputId": "f11bc077-b35a-4289-f431-4d3ee147019a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens: ['Natural', 'Language', 'Processing', 'amazing', '!', 'Let', \"'s\", 'explore', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1.3 Lemmatization and Stemming**\n",
        "bold text\n",
        "**Using WordNet Lemmatizer and Porter Stemmer.**"
      ],
      "metadata": {
        "id": "uMifCQ2psiug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
        "\n",
        "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n",
        "print(\"Stemmed Tokens:\", stemmed_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f39HLK66-7l3",
        "outputId": "bc32cd98-b19d-4fe7-d9c3-6b0b56d6eec2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens: ['Natural', 'Language', 'Processing', 'amazing', '!', 'Let', \"'s\", 'explore', '.']\n",
            "Stemmed Tokens: ['natur', 'languag', 'process', 'amaz', '!', 'let', \"'s\", 'explor', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step 2: Feature Extraction**\n",
        "\n",
        "Transforming text into numerical features for model input.\n",
        "\n",
        "# **2.1 Bag-of-Words (BoW)**\n",
        "\n",
        "Using CountVectorizer to convert text into a matrix of token counts. **bold text**"
      ],
      "metadata": {
        "id": "BvVT9LrZspdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.2 TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "\n",
        "Measuring the importance of words in a document. **bold text**"
      ],
      "metadata": {
        "id": "45w26JdMs4t5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\"NLP is fun and exciting.\", \"Machine learning enhances NLP capabilities.\"]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"Feature Names:\", vectorizer.get_feature_names_out())\n",
        "print(\"BoW Representation:\\n\", X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5X6cwAB7_B63",
        "outputId": "cd3f2601-60da-47f2-8aaf-309d9d120250"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Names: ['and' 'capabilities' 'enhances' 'exciting' 'fun' 'is' 'learning'\n",
            " 'machine' 'nlp']\n",
            "BoW Representation:\n",
            " [[1 0 0 1 1 1 0 0 1]\n",
            " [0 1 1 0 0 0 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Building and Training NLP Models\n",
        "\n",
        "# 3.1 Sentiment Analysis with Naive Bayes **bold text**"
      ],
      "metadata": {
        "id": "Aedv6CcJs--h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(\"TF-IDF Representation:\\n\", X_tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndi9cWul_INa",
        "outputId": "66cd6045-9bc5-42bc-c80f-ee4a68938171"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Representation:\n",
            " [[0.47107781 0.         0.         0.47107781 0.47107781 0.47107781\n",
            "  0.         0.         0.33517574]\n",
            " [0.         0.47107781 0.47107781 0.         0.         0.\n",
            "  0.47107781 0.47107781 0.33517574]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Model Evaluation\n",
        "\n",
        "# 4.1 Classification Metrics\n",
        "\n",
        "Evaluating model performance using Precision, Recall, F1-score, and Confusion Matrix.**bold text** **bold text**"
      ],
      "metadata": {
        "id": "n7eGm4kktKBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example dataset\n",
        "data = [\n",
        "    (\"I love this product!\", \"positive\"),\n",
        "    (\"This is the worst experience ever.\", \"negative\"),\n",
        "    (\"It's okay, not great but not terrible.\", \"neutral\"),\n",
        "]\n",
        "\n",
        "texts, labels = zip(*data)\n",
        "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create pipeline\n",
        "model = Pipeline([\n",
        "    ('vectorizer', TfidfVectorizer()),\n",
        "    ('classifier', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict\n",
        "predictions = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aql0TKSN_UNv",
        "outputId": "5fd26c12-ae7e-40d7-8357-c4d375505bc2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Confusion Matrix (Error Analysis)**"
      ],
      "metadata": {
        "id": "9e6NevntuWhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, predictions))\n",
        "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7b4X3_l_bXe",
        "outputId": "4a698048-8790-4dcb-f5f9-84bd0a51d318"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.00      0.00      0.00       0.0\n",
            "    positive       0.00      0.00      0.00       1.0\n",
            "\n",
            "    accuracy                           0.00       1.0\n",
            "   macro avg       0.00      0.00      0.00       1.0\n",
            "weighted avg       0.00      0.00      0.00       1.0\n",
            "\n",
            "Confusion Matrix:\n",
            " [[0 0]\n",
            " [1 0]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Named Entity Recognition (NER)**"
      ],
      "metadata": {
        "id": "psfb8IqkuNf5"
      }
    },
    {
      "source": [
        "import spacy # Make sure you have installed spacy with `!pip install spacy`\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(\"Apple is looking at buying a U.K. startup for $1 billion.\")\n",
        "\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, \"-\", ent.label_)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3xlipap6_kjR",
        "outputId": "a94884c1-b471-4041-fcab-ae25f18e097e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Apple - ORG\n",
            "U.K. - GPE\n",
            "$1 billion - MONEY\n"
          ]
        }
      ]
    }
  ]
}